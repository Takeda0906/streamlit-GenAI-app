import streamlit as st
import tiktoken

# ==== æœ€æ–° LangChain 1.x ç”¨ Import ====
from langchain.chat_models.openai import ChatOpenAI
from langchain.chat_models.anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage

# ==== ãƒ¢ãƒ‡ãƒ«åˆ¥ä¾¡æ ¼è¨­å®šï¼ˆUSD/1M tokensï¼‰ ====
MODEL_PRICES = {
    "input": {
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o": 5 / 1_000_000,
        "gpt-5": 10 / 1_000_000,
        "gpt-5-mini": 2 / 1_000_000,
        "claude-3-haiku-20240307": 0.25 / 1_000_000,
        "gemini-2.5-pro": 3.5 / 1_000_000,
        "gemini-2.5-flash": 0.075 / 1_000_000
    },
    "output": {
        "gpt-3.5-turbo": 1.5 / 1_000_000,
        "gpt-4o": 15 / 1_000_000,
        "gpt-5": 30 / 1_000_000,
        "gpt-5-mini": 6 / 1_000_000,
        "claude-3-haiku-20240307": 1.25 / 1_000_000,
        "gemini-2.5-pro": 10.5 / 1_000_000,
        "gemini-2.5-flash": 0.30 / 1_000_000
    }
}

# ==== ãƒšãƒ¼ã‚¸åˆæœŸåŒ– ====
def init_page():
    st.set_page_config(page_title="AI Chat App", page_icon="ğŸ¤–")
    st.header("AI Chat App ğŸ¤–")
    st.sidebar.title("è¨­å®š")

# ==== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´åˆæœŸåŒ– ====
def init_messages():
    if "message_history" not in st.session_state:
        st.session_state.message_history = [("system", "You are a helpful assistant.")]
    if st.sidebar.button("ğŸ’¬ ä¼šè©±ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.message_history = [("system", "You are a helpful assistant.")]

# ==== ãƒ¢ãƒ‡ãƒ«é¸æŠ ====
def select_model():
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    if "model_choice" not in st.session_state:
        st.session_state.model_choice = "GPT-3.5"
    if "temperature" not in st.session_state:
        st.session_state.temperature = 0.7

    # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®å€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ä¿æŒ
    model_options = ["GPT-3.5", "GPT-4", "GPT-5", "GPT-5 Mini",
                     "Claude 3 Haiku", "Gemini 2.5 Pro", "Gemini 2.5 Flash"]

    model_choice = st.sidebar.radio(
        "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:",
        model_options,
        index=model_options.index(st.session_state.model_choice)
    )

    st.session_state.model_choice = model_choice

    # æ¸©åº¦è¨­å®š
    if model_choice in ["GPT-5", "GPT-5 Mini"]:
        st.sidebar.info("âš  GPT-5 ç³»ãƒ¢ãƒ‡ãƒ«ã¯å›ºå®šæ¸©åº¦ 1 ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")
        temperature = 1.0
    elif "Claude" in model_choice:
        temperature = float(st.sidebar.slider("æ¸©åº¦ (å‰µé€ æ€§):", 0.0, 1.0, st.session_state.temperature, 0.01))
    else:
        temperature = float(st.sidebar.slider("æ¸©åº¦ (å‰µé€ æ€§):", 0.0, 2.0, st.session_state.temperature, 0.01))

    st.session_state.temperature = temperature

    # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    try:
        if model_choice == "GPT-3.5":
            st.session_state.model_name = "gpt-3.5-turbo"
            return ChatOpenAI(model_name="gpt-3.5-turbo", temperature=temperature)
        elif model_choice == "GPT-4":
            st.session_state.model_name = "gpt-4o"
            return ChatOpenAI(model_name="gpt-4o", temperature=temperature)
        elif model_choice == "GPT-5":
            st.session_state.model_name = "gpt-5"
            return ChatOpenAI(model_name="gpt-5", temperature=temperature)
        elif model_choice == "GPT-5 Mini":
            st.session_state.model_name = "gpt-5-mini"
            return ChatOpenAI(model_name="gpt-5-mini", temperature=temperature)
        elif model_choice == "Claude 3 Haiku":
            st.session_state.model_name = "claude-3-haiku-20240307"
            return ChatAnthropic(model="claude-3-haiku-20240307", temperature=temperature)
        elif model_choice == "Gemini 2.5 Pro":
            st.session_state.model_name = "gemini-2.5-pro"
            return ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=temperature)
        elif model_choice == "Gemini 2.5 Flash":
            st.session_state.model_name = "gemini-2.5-flash"
            return ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=temperature)
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
        return None

# ==== ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®— ====
def get_token_count(text, model_name):
    if "gemini" in model_name:
        return len(text) // 2
    else:
        encoding = tiktoken.encoding_for_model(model_name if "gpt" in model_name else "gpt-3.5-turbo")
        return len(encoding.encode(text))

# ==== ã‚³ã‚¹ãƒˆè¨ˆç®— ====
def calc_and_display_costs():
    input_count = 0
    output_count = 0
    for role, message in st.session_state.message_history:
        token_count = get_token_count(message, st.session_state.model_name)
        if role == "ai":
            output_count += token_count
        else:
            input_count += token_count

    if len(st.session_state.message_history) <= 1:
        return

    model = st.session_state.model_name
    input_cost = MODEL_PRICES["input"].get(model, 0) * input_count
    output_cost = MODEL_PRICES["output"].get(model, 0) * output_count
    total_cost = input_cost + output_cost

    st.sidebar.markdown("## ğŸ’° ã‚³ã‚¹ãƒˆè©¦ç®—")
    st.sidebar.markdown(f"**åˆè¨ˆã‚³ã‚¹ãƒˆ:** ${total_cost:.5f}")
    st.sidebar.markdown(f"- å…¥åŠ›ã‚³ã‚¹ãƒˆ: ${input_cost:.5f}")
    st.sidebar.markdown(f"- å‡ºåŠ›ã‚³ã‚¹ãƒˆ: ${output_cost:.5f}")

# ==== ãƒ¡ã‚¤ãƒ³å‡¦ç† ====
def main():
    init_page()
    init_messages()

    if "llm" not in st.session_state or st.session_state.llm is None:
        st.session_state.llm = select_model()

    # å±¥æ­´è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)

    # å…¥åŠ›å—ä»˜
    user_input = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")
    if user_input and st.session_state.llm:
        st.chat_message("user").markdown(user_input)

        try:
            if "gemini" in st.session_state.model_name:
                response = st.session_state.llm.invoke([{"role": "user", "content": user_input}]).content
            elif "claude" in st.session_state.model_name:
                response = st.session_state.llm.invoke(user_input).content
            else:
                messages_for_gpt = [
                    HumanMessage(content=content) if role == "user" else
                    AIMessage(content=content) if role in ["assistant", "ai"] else
                    SystemMessage(content=content)
                    for role, content in st.session_state.message_history
                ]
                messages_for_gpt.append(HumanMessage(content=user_input))
                response = st.session_state.llm.invoke(messages_for_gpt).content

            st.chat_message("ai").markdown(response)

            # å±¥æ­´æ›´æ–°
            st.session_state.message_history.append(("user", user_input))
            st.session_state.message_history.append(("ai", response))

        except Exception as e:
            st.error(f"LLM å¿œç­”ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {e}")

    # ã‚³ã‚¹ãƒˆè¡¨ç¤º
    calc_and_display_costs()

if __name__ == "__main__":
    main()